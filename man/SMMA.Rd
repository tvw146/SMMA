% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/SMMA.R
\name{SMMA}
\alias{SMMA}
\alias{softmaximin}
\title{Maximin Estimation for Large Scale Array Data with Known Groups}
\usage{
softmaximin(X, 
            Y, 
            penalty = c("lasso", "scad"),
            nlambda = 100,
            lambda.min.ratio = 1e-04,
            lambda = NULL,
            penalty.factor = NULL,
            reltol = 1e-07,
            maxiter = 15000,
            steps = 1,
            btmax = 100,
            zeta = 2, 
            c = 0.001
            Delta0 = 1,
            nu = 1,
            alg = c("npg", "mfista"))
}
\arguments{
\item{X}{A list containing the Kronecker components (2 or 3) of the Kronecker design matrix.
These are  matrices of sizes \eqn{n_i \times p_i}.}

\item{Y}{The response values, an array of size \eqn{n_1 \times\cdots\times n_d \times G}.}

\item{penalty}{A string specifying the penalty. Possible values 
are \code{"lasso", "scad"}.}

\item{nlambda}{The number of \code{lambda} values.}

\item{lambda.min.ratio}{The smallest value for \code{lambda}, given as a fraction of 
\eqn{\lambda_{max}}; the (data derived) smallest value for which all coefficients are zero.}

\item{lambda}{The sequence of penalty parameters for the regularization path.}

\item{penalty.factor}{An array of size \eqn{p_1 \times \cdots \times p_d}. Is multiplied 
with each element in \code{lambda} to allow differential shrinkage on the coefficients.}

\item{reltol}{The convergence tolerance for the inner loop.}

\item{maxiter}{The maximum number of  iterations allowed for each \code{lambda}
value, when  summing over all outer iterations for said \code{lambda}.}

\item{steps}{The number of steps used in the multi-step adaptive lasso algorithm for non-convex penalties. 
Automatically set to 1 when \code{penalty = "lasso"}.}

\item{btmax}{Maximum number of backtracking steps allowed in each iteration. Default is \code{btmax = 100}.}

\item{zeta}{Constant controlling  the softmax apprximation accuracy. Must be strictly positive. Default is \code{zeta = 2}.}

\item{c}{constant used in the NPG algorithm. Must be strictly positive. Default is \code{c = 0.001}.}

\item{Delta0}{constant used to bound the stepsize. Must be strictly positive. Default is \code{Delta0 = 1}.}

\item{nu}{constant used to control the stepsize. Must be positive. A small value gives a big stepsize. Default is \code{nu = 1}.}

\item{alg}{string indicating which algortihm to use. Possible values are \code{"npg", "mfista"}.}
}
\value{
An object with S3 Class "SMMA". 
\item{spec}{A string indicating the array dimension (2 or 3) and the penalty.}  
\item{coef}{A \eqn{p_1\cdots p_d \times} \code{nlambda} matrix containing the estimates of 
the model coefficients (\code{beta}) for each \code{lambda}-value.}
\item{lambda}{A vector containing the sequence of penalty values used in the estimation procedure.}
\item{Obj}{A matrix containing the objective values for each iteration and each model.}
\item{df}{The number of nonzero coefficients for each value of \code{lambda}.}	
\item{dimcoef}{A vector giving the dimension of the model coefficient array \eqn{\beta}.}
\item{dimobs}{A vector giving the dimension of the observation (response) array \code{Y}.}
\item{Iter}{A list with 4 items:  
\code{bt_iter}  is total number of backtracking steps performed,
\code{bt_enter} is the number of times the backtracking is initiated,
and \code{iter_mat} is a vector containing the  number of  iterations for each \code{lambda} value 
and  \code{iter} is total number of iterations i.e. \code{sum(Iter)}.}
}
\description{
Efficient design matrix free procedure for soving the soft maximin problem for 
large scale penalized  2 or 3-dimensional generalized linear array models. Currently the LASSO penalty 
and the SCAD penalty are both implemented, see  \cite{Lund et al., 2017}.
}
\details{
We consider the maximin effects estimator for array data with a known fixed group structure 
and tensor structured design matrix. 

For  \eqn{g \in \{1,\ldots,G\}} let \eqn{n} be the number of observations in each group. 
With  \eqn{Y_g:=(y_{i},\ldots,y_{i_{n}})^\top}  the   group-specific $n_1\times \cdots \times n_d$ response array  
and \eqn{X :=(x_{i}\mid\ldots\mid x_{i_{n}})^\top} a \eqn{n\times p} design matrix, with tensor structure
 \deqn{X = \bigotimes_{i=1}^d X_i,} where \eqn{X_1,X_2,\ldots} are the marginal \eqn{n_i\times p_i} design matrices (Kronecker components). 
 
We use the generalized linear array model (GLAM) framework to write the model equation as
 \deqn{Y_g = \rho(X_d,\rho(X_{d-1},\ldots,\rho(X_1,B_g))) + E,}
where \eqn{\rho} is the so called rotated \eqn{H}-transfrom, \eqn{B_g} is the   random parameter array
 and \eqn{E} some additive noise that is uncorrelated with \eqn{X}. See \cite{Lund et al., 2017} for more details.
        
Using only the marginal matrices \eqn{X_1,X_2,\ldots} and with \eqn{J} a penalty function, the function \code{glamlasso} 
solves the penalized softmaximin problem 
\deqn{\min_{\theta} \sum_{g=1}^G \exp{-\zeta \hat V_g(\beta)} + \lambda J (\beta),} 
in this setup for a sequence of penalty parameters \eqn{\lambda>0}. The underlying algorithm is based on an  
proximal gradient based loop. We note that if \eqn{J} is not  convex, as with the SCAD penalty,
we use the multiple step adaptive lasso procedure to loop over the inner proximal algorithm, see \cite{Lund et al., 2017} for more details.
}
\examples{
\dontrun{
##size of example 
n1 <- 65; n2 <- 26; n3 <- 13; p1 <- 13; p2 <- 5; p3 <- 4

##marginal design matrices (Kronecker components)
X1 <- matrix(rnorm(n1 * p1), n1, p1) 
X2 <- matrix(rnorm(n2 * p2), n2, p2) 
X3 <- matrix(rnorm(n3 * p3), n3, p3) 
X <- list(X1, X2, X3)

##gaussian example 
Beta <- array(rnorm(p1 * p2 * p3) * rbinom(p1 * p2 * p3, 1, 0.1), c(p1 , p2, p3))
mu <- RH(X3, RH(X2, RH(X1, Beta)))
Y <- array(rnorm(n1 * n2 * n3, mu), dim = c(n1, n2, n3))

fit <- softmaximin(X, Y, family = "gaussian", penalty = "lasso", iwls = "exact")
Betafit <- fit$coef

modelno <- length(fit$lambda)
m <- min(Betafit[ , modelno], c(Beta))
M <- max(Betafit[ , modelno], c(Beta))
plot(c(Beta), type="l", ylim = c(m, M))
lines(Betafit[ , modelno], col = "red")
}
}
\author{
Adam Lund

Maintainer: Adam Lund, \email{adam.lund@math.ku.dk}
}
\references{
Lund, A.,  (2017). .......
\emph{ArXiv}.
}
\keyword{package}

